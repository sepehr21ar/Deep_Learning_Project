{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "301764e0",
   "metadata": {},
   "source": [
    "Predicting house prices using Image and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f229ae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras import Model, Input\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dense, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "import os\n",
    "import locale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ceb0b215",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'Houses-dataset' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/emanhamed/Houses-dataset.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4dd6f016",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputPath = \"C:\\\\Users\\\\Sepehr\\\\ML_file\\\\Deep-Learning\\\\BDI\\\\Houses-dataset\\\\Houses_Dataset\\\\HousesInfo.txt\"\n",
    "datasetPath = \"C:\\\\Users\\\\Sepehr\\\\ML_file\\\\Deep-Learning\\\\BDI\\\\Houses-dataset\\\\Houses_Dataset\"\n",
    "\n",
    "cols = [\"bedrooms\", \"bathrooms\", \"area\", \"zipcode\", \"price\"]\n",
    "df = pd.read_csv(inputPath, sep=\" \", header=None, names=cols)\n",
    "\n",
    "\n",
    "zipcodes, counts = np.unique(df[\"zipcode\"], return_counts=True)\n",
    "\n",
    "# loop over each of the unique zip codes and their corresponding\n",
    "# count\n",
    "for (zipcode, count) in zip(zipcodes, counts):\n",
    "    # the zip code counts for our housing dataset is *extremely*\n",
    "    # unbalanced (some only having 1 or 2 houses per zip code)\n",
    "    # so let's sanitize our data by removing any houses with less\n",
    "    # than 25 houses per zip code\n",
    "    if count < 25:\n",
    "        idxs = df[df[\"zipcode\"] == zipcode].index\n",
    "        df.drop(idxs, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "816c863e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22c67870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our images array (i.e., the house images themselves)\n",
    "images = []\n",
    "\n",
    "# loop over the indexes of the houses\n",
    "for i in df.index.values:\n",
    "    # find the four images for the house and sort the file paths,\n",
    "    # ensuring the four are always in the *same order*\n",
    "    basePath = os.path.sep.join([datasetPath, \"{}_*\".format(i + 1)])\n",
    "    housePaths = sorted(list(glob.glob(basePath)))\n",
    "    # initialize our list of input images along with the output image\n",
    "    # after *combining* the four input images\n",
    "    inputImages = []\n",
    "    outputImage = np.zeros((64, 64, 3), dtype=\"uint8\")\n",
    "\n",
    "    # loop over the input house paths\n",
    "    for housePath in housePaths:\n",
    "        # load the input image, resize it to be 32 32, and then\n",
    "        # update the list of input images\n",
    "        image = cv2.imread(housePath)\n",
    "        image = cv2.resize(image, (32, 32))\n",
    "        inputImages.append(image)\n",
    "\n",
    "    # tile the four input images in the output image such the first\n",
    "    # image goes in the top-right corner, the second image in the\n",
    "    # top-left corner, the third image in the bottom-right corner,\n",
    "    # and the final image in the bottom-left corner\n",
    "    outputImage[0:32, 0:32] = inputImages[0]\n",
    "    outputImage[0:32, 32:64] = inputImages[1]\n",
    "    outputImage[32:64, 32:64] = inputImages[2]\n",
    "    outputImage[32:64, 0:32] = inputImages[3]\n",
    "\n",
    "    # add the tiled image to our set of images the network will be\n",
    "    # trained on\n",
    "    images.append(outputImage)\n",
    "images = np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f73fb7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "51e5b0e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[153, 168, 184],\n",
       "        [158, 175, 191],\n",
       "        [155, 173, 190],\n",
       "        ...,\n",
       "        [ 92,  99, 116],\n",
       "        [140, 153, 169],\n",
       "        [132, 144, 160]],\n",
       "\n",
       "       [[160, 176, 193],\n",
       "        [159, 176, 194],\n",
       "        [156, 173, 194],\n",
       "        ...,\n",
       "        [ 29,  50,  93],\n",
       "        [137, 149, 168],\n",
       "        [139, 152, 169]],\n",
       "\n",
       "       [[162, 178, 196],\n",
       "        [160, 176, 195],\n",
       "        [157, 174, 197],\n",
       "        ...,\n",
       "        [ 80, 118, 173],\n",
       "        [137, 151, 169],\n",
       "        [142, 158, 176]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[161, 175, 193],\n",
       "        [161, 165, 177],\n",
       "        [148, 162, 180],\n",
       "        ...,\n",
       "        [ 17,  50, 116],\n",
       "        [167, 173, 185],\n",
       "        [193, 194, 208]],\n",
       "\n",
       "       [[159, 173, 191],\n",
       "        [151, 166, 182],\n",
       "        [146, 158, 177],\n",
       "        ...,\n",
       "        [ 40,  50,  83],\n",
       "        [153, 160, 177],\n",
       "        [195, 194, 206]],\n",
       "\n",
       "       [[157, 172, 188],\n",
       "        [151, 166, 182],\n",
       "        [161, 174, 190],\n",
       "        ...,\n",
       "        [ 19,  28,  41],\n",
       "        [ 85,  92, 148],\n",
       "        [193, 187, 198]]], dtype=uint8)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "198cbae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition the data into training and testing splits using 75% of\n",
    "# the data for training and the remaining 25% for testing\n",
    "split = train_test_split(df, images, test_size=0.25, random_state=42)\n",
    "(trainAttrX, testAttrX, trainImagesX, testImagesX) = split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81c30cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the largest house price in the training set and use it to\n",
    "# scale our house prices to the range [0, 1] (will lead to better\n",
    "# training and convergence)\n",
    "maxPrice = trainAttrX[\"price\"].max()\n",
    "trainY = trainAttrX[\"price\"] / maxPrice\n",
    "testY = testAttrX[\"price\"] / maxPrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a9530fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the column names of the continuous data\n",
    "continuous = [\"bedrooms\", \"bathrooms\", \"area\"]\n",
    "\n",
    "# performin min-max scaling each continuous feature column to\n",
    "# the range [0, 1]\n",
    "cs = MinMaxScaler()\n",
    "trainContinuous = cs.fit_transform(trainAttrX[continuous])\n",
    "testContinuous = cs.transform(testAttrX[continuous])\n",
    "\n",
    "# one-hot encode the zip code categorical data (by definition of\n",
    "# one-hot encoing, all output features are now in the range [0, 1])\n",
    "zipBinarizer = LabelBinarizer().fit(df[\"zipcode\"])\n",
    "trainCategorical = zipBinarizer.transform(trainAttrX[\"zipcode\"])\n",
    "testCategorical = zipBinarizer.transform(testAttrX[\"zipcode\"])\n",
    "\n",
    "# construct our training and testing data points by concatenating\n",
    "# the categorical features with the continuous features\n",
    "trainAttrX = np.hstack([trainCategorical, trainContinuous])\n",
    "testAttrX = np.hstack([testCategorical, testContinuous])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2277a42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = trainAttrX.shape[1] #10\n",
    "\n",
    "# define our MLP network\n",
    "inp = keras.Input(shape=(dim,))\n",
    "x = Dense(16, activation=\"relu\")(inp)\n",
    "x = Dense(7, activation=\"relu\")(x)\n",
    "out = Dense(4, activation=\"relu\")(x)\n",
    "\n",
    "model = keras.Model(inp, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "870921c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "width, height, depth = 64, 64, 3\n",
    "filters=(16, 32, 64)\n",
    "# initialize the input shape and channel dimension, assuming\n",
    "# TensorFlow/channels-last ordering\n",
    "inputShape = (height, width, depth)\n",
    "\n",
    "# define the model input\n",
    "inputs = Input(shape=inputShape)\n",
    "\n",
    "# loop over the number of filters\n",
    "for (i, f) in enumerate(filters):\n",
    "    # if this is the first CONV layer then set the input\n",
    "    # appropriately\n",
    "    if i == 0:\n",
    "        x = inputs\n",
    "\n",
    "    # CONV => RELU => BN => POOL\n",
    "    x = Conv2D(f, (3, 3), padding=\"same\")(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# flatten the volume, then FC => RELU => BN => DROPOUT\n",
    "x = Flatten()(x)\n",
    "x = Dense(16)(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "# apply another FC layer, this one to match the number of nodes\n",
    "# coming out of the MLP\n",
    "x = Dense(4)(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "\n",
    "# construct the CNN\n",
    "cnn = Model(inputs, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5b8dec9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import concatenate\n",
    "# create the input to our final set of layers as the *output* of both\n",
    "# the MLP and CNN\n",
    "combinedInput = concatenate([model.output, cnn.output])\n",
    "\n",
    "# our final FC layer head will have two dense layers, the final one\n",
    "# being our regression head\n",
    "x = Dense(4, activation=\"relu\")(combinedInput)\n",
    "x = Dense(1, activation=\"linear\")(x)\n",
    "\n",
    "model = Model([model.input, cnn.input], x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "73f5a655",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mean_absolute_percentage_error\", optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b10dbe8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) for `plot_model` to work.\n"
     ]
    }
   ],
   "source": [
    "keras.utils.plot_model(model, show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b4b862e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sepehr\\miniconda3\\envs\\ml_env\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor_27', 'keras_tensor_31']. Received: the structure of inputs=('*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 1142.7137"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sepehr\\miniconda3\\envs\\ml_env\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor_27', 'keras_tensor_31']. Received: the structure of inputs=('*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 46ms/step - loss: 1135.7834 - val_loss: 82.5456 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 411.3386 - val_loss: 69.8695 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 317.1809 - val_loss: 64.1474 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 260.6678 - val_loss: 60.1161 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 209.5428 - val_loss: 58.5568 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 216.3258 - val_loss: 97.3168 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 157.2444 - val_loss: 240.3195 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 241.2546 - val_loss: 401.0251 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 220.1433 - val_loss: 528.7836 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 106.7582 - val_loss: 789.6307 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 117.8641 - val_loss: 920.6666 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 119.0083 - val_loss: 1109.4683 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 99.2876 - val_loss: 864.5574 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 84.3364 - val_loss: 613.8076 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 71.5023 - val_loss: 320.5964 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 69.9457 - val_loss: 179.3898 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 67.3618 - val_loss: 102.6395 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 58.0022 - val_loss: 104.5206 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 57.4277 - val_loss: 223.0469 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 40.1145 - val_loss: 92.3542 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 38.8720 - val_loss: 79.2279 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 42.0267 - val_loss: 77.7259 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 36.5391 - val_loss: 66.5969 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 29.5347 - val_loss: 26.6971 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 28.4356 - val_loss: 25.0686 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 37.4098 - val_loss: 22.0732 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 27.8727 - val_loss: 23.0236 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 23.4693 - val_loss: 21.9239 - learning_rate: 0.0010\n",
      "Epoch 29/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 30.8352 - val_loss: 22.7918 - learning_rate: 0.0010\n",
      "Epoch 30/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - loss: 27.3789 - val_loss: 21.2671 - learning_rate: 0.0010\n",
      "Epoch 31/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 37.4576 - val_loss: 27.6089 - learning_rate: 0.0010\n",
      "Epoch 32/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 25.3811 - val_loss: 20.7209 - learning_rate: 0.0010\n",
      "Epoch 33/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 29.1994 - val_loss: 20.6249 - learning_rate: 0.0010\n",
      "Epoch 34/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step - loss: 25.8791 - val_loss: 23.6934 - learning_rate: 0.0010\n",
      "Epoch 35/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - loss: 30.9168 - val_loss: 22.2880 - learning_rate: 0.0010\n",
      "Epoch 36/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - loss: 29.4982 - val_loss: 20.6601 - learning_rate: 0.0010\n",
      "Epoch 37/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 24.1240 - val_loss: 19.7648 - learning_rate: 0.0010\n",
      "Epoch 38/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step - loss: 28.4996 - val_loss: 20.5955 - learning_rate: 0.0010\n",
      "Epoch 39/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - loss: 25.3974 - val_loss: 19.6292 - learning_rate: 0.0010\n",
      "Epoch 40/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - loss: 30.2316 - val_loss: 22.6225 - learning_rate: 0.0010\n",
      "Epoch 41/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - loss: 26.0917 - val_loss: 21.6621 - learning_rate: 0.0010\n",
      "Epoch 42/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - loss: 25.2821 - val_loss: 21.5183 - learning_rate: 0.0010\n",
      "Epoch 43/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 24.3924 - val_loss: 22.0311 - learning_rate: 0.0010\n",
      "Epoch 44/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - loss: 24.0232 - val_loss: 20.2620 - learning_rate: 0.0010\n",
      "Epoch 45/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - loss: 26.0644 - val_loss: 23.2819 - learning_rate: 0.0010\n",
      "Epoch 46/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 54ms/step - loss: 27.1190 - val_loss: 20.7801 - learning_rate: 0.0010\n",
      "Epoch 47/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 25.8357 - val_loss: 19.8893 - learning_rate: 0.0010\n",
      "Epoch 48/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - loss: 23.9087 - val_loss: 21.0394 - learning_rate: 0.0010\n",
      "Epoch 49/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - loss: 30.8907 - val_loss: 23.9600 - learning_rate: 0.0010\n",
      "Epoch 50/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 24.5159 - val_loss: 23.9358 - learning_rate: 0.0010\n",
      "Epoch 51/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - loss: 22.4230 - val_loss: 19.4200 - learning_rate: 0.0010\n",
      "Epoch 52/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 22.6146 - val_loss: 22.4490 - learning_rate: 0.0010\n",
      "Epoch 53/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 24.5564 - val_loss: 20.7560 - learning_rate: 0.0010\n",
      "Epoch 54/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 25.9124 - val_loss: 21.2222 - learning_rate: 0.0010\n",
      "Epoch 55/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 22.2250 - val_loss: 23.6067 - learning_rate: 0.0010\n",
      "Epoch 56/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step - loss: 20.2239 - val_loss: 20.8227 - learning_rate: 0.0010\n",
      "Epoch 57/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 29.8421 - val_loss: 27.9065 - learning_rate: 0.0010\n",
      "Epoch 58/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 27.1273 - val_loss: 23.0592 - learning_rate: 0.0010\n",
      "Epoch 59/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 27.5246 - val_loss: 21.5688 - learning_rate: 0.0010\n",
      "Epoch 60/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 24.4481 - val_loss: 24.3897 - learning_rate: 0.0010\n",
      "Epoch 61/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - loss: 29.1473 - val_loss: 19.4118 - learning_rate: 0.0010\n",
      "Epoch 62/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 22.5671 - val_loss: 22.1934 - learning_rate: 0.0010\n",
      "Epoch 63/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 24.7821 - val_loss: 23.1981 - learning_rate: 0.0010\n",
      "Epoch 64/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 24.9328 - val_loss: 19.8825 - learning_rate: 0.0010\n",
      "Epoch 65/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 22.6549 - val_loss: 23.2753 - learning_rate: 0.0010\n",
      "Epoch 66/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 20.7615 - val_loss: 20.5494 - learning_rate: 0.0010\n",
      "Epoch 67/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - loss: 25.6610 - val_loss: 25.2484 - learning_rate: 0.0010\n",
      "Epoch 68/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step - loss: 23.3445 - val_loss: 19.0498 - learning_rate: 0.0010\n",
      "Epoch 69/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 23.7097 - val_loss: 20.1442 - learning_rate: 0.0010\n",
      "Epoch 70/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 24.3036 - val_loss: 24.7160 - learning_rate: 0.0010\n",
      "Epoch 71/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - loss: 24.2626 - val_loss: 24.4215 - learning_rate: 0.0010\n",
      "Epoch 72/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 23.5013 - val_loss: 20.6694 - learning_rate: 0.0010\n",
      "Epoch 73/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - loss: 22.7433 - val_loss: 20.5705 - learning_rate: 0.0010\n",
      "Epoch 74/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 20.4787 - val_loss: 22.3927 - learning_rate: 0.0010\n",
      "Epoch 75/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step - loss: 24.5200 - val_loss: 20.4652 - learning_rate: 0.0010\n",
      "Epoch 76/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - loss: 28.9037 - val_loss: 29.8957 - learning_rate: 0.0010\n",
      "Epoch 77/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 20.8227 - val_loss: 26.0572 - learning_rate: 0.0010\n",
      "Epoch 78/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - loss: 25.2415 - val_loss: 27.0527 - learning_rate: 0.0010\n",
      "Epoch 79/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - loss: 22.6236 - val_loss: 22.5244 - learning_rate: 0.0010\n",
      "Epoch 80/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 47ms/step - loss: 19.4129 - val_loss: 21.3825 - learning_rate: 0.0010\n",
      "Epoch 81/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - loss: 21.6330 - val_loss: 21.3139 - learning_rate: 0.0010\n",
      "Epoch 82/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 20.5463 - val_loss: 20.9971 - learning_rate: 0.0010\n",
      "Epoch 83/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 23.6341 - val_loss: 24.6943 - learning_rate: 0.0010\n",
      "Epoch 84/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - loss: 19.7825 - val_loss: 37.1514 - learning_rate: 0.0010\n",
      "Epoch 85/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - loss: 25.5092 - val_loss: 27.7028 - learning_rate: 0.0010\n",
      "Epoch 86/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step - loss: 24.7484 - val_loss: 21.9468 - learning_rate: 0.0010\n",
      "Epoch 87/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - loss: 21.5079 - val_loss: 20.1583 - learning_rate: 0.0010\n",
      "Epoch 88/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - loss: 23.7351 - val_loss: 20.1654 - learning_rate: 0.0010\n",
      "Epoch 89/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 54ms/step - loss: 21.4999 - val_loss: 20.8742 - learning_rate: 0.0010\n",
      "Epoch 90/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - loss: 21.3497 - val_loss: 20.6493 - learning_rate: 0.0010\n",
      "Epoch 91/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - loss: 24.3052 - val_loss: 23.7859 - learning_rate: 0.0010\n",
      "Epoch 92/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - loss: 24.7218 - val_loss: 23.4898 - learning_rate: 0.0010\n",
      "Epoch 93/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - loss: 22.5232 - val_loss: 22.9814 - learning_rate: 0.0010\n",
      "Epoch 94/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - loss: 25.5366 - val_loss: 20.5745 - learning_rate: 0.0010\n",
      "Epoch 95/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step - loss: 19.7532 - val_loss: 20.7765 - learning_rate: 0.0010\n",
      "Epoch 96/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step - loss: 22.5981 - val_loss: 19.7795 - learning_rate: 0.0010\n",
      "Epoch 97/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 20.9785 - val_loss: 25.3221 - learning_rate: 0.0010\n",
      "Epoch 98/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - loss: 29.6712 - val_loss: 20.8776 - learning_rate: 0.0010\n",
      "Epoch 99/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 20.2528 - val_loss: 20.2705 - learning_rate: 0.0010\n",
      "Epoch 100/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 18.7351 - val_loss: 29.0633 - learning_rate: 0.0010\n",
      "Epoch 101/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 26.5651 - val_loss: 27.0586 - learning_rate: 0.0010\n",
      "Epoch 102/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 22.9208 - val_loss: 22.5078 - learning_rate: 0.0010\n",
      "Epoch 103/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 21.2677 - val_loss: 20.3422 - learning_rate: 0.0010\n",
      "Epoch 104/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 19.7759 - val_loss: 20.2111 - learning_rate: 1.0000e-04\n",
      "Epoch 105/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 18.2121 - val_loss: 20.0516 - learning_rate: 1.0000e-04\n",
      "Epoch 106/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 19.1843 - val_loss: 20.2711 - learning_rate: 1.0000e-04\n",
      "Epoch 107/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 19.2495 - val_loss: 20.3229 - learning_rate: 1.0000e-04\n",
      "Epoch 108/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - loss: 20.4722 - val_loss: 20.2688 - learning_rate: 1.0000e-04\n",
      "Epoch 109/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - loss: 18.9580 - val_loss: 20.2903 - learning_rate: 1.0000e-04\n",
      "Epoch 110/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 18.9133 - val_loss: 20.5431 - learning_rate: 1.0000e-04\n",
      "Epoch 111/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 18.4540 - val_loss: 20.2968 - learning_rate: 1.0000e-04\n",
      "Epoch 112/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 19.9849 - val_loss: 20.3302 - learning_rate: 1.0000e-04\n",
      "Epoch 113/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - loss: 19.0771 - val_loss: 20.7074 - learning_rate: 1.0000e-04\n",
      "Epoch 114/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 18.2574 - val_loss: 21.0948 - learning_rate: 1.0000e-04\n",
      "Epoch 115/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 19.6895 - val_loss: 20.1754 - learning_rate: 1.0000e-04\n",
      "Epoch 116/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 47ms/step - loss: 20.0155 - val_loss: 20.2939 - learning_rate: 1.0000e-04\n",
      "Epoch 117/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 19.9856 - val_loss: 19.9979 - learning_rate: 1.0000e-04\n",
      "Epoch 118/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 20.8487 - val_loss: 20.4768 - learning_rate: 1.0000e-04\n",
      "Epoch 119/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 20.1575 - val_loss: 19.9989 - learning_rate: 1.0000e-04\n",
      "Epoch 120/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 20.1986 - val_loss: 20.2562 - learning_rate: 1.0000e-04\n",
      "Epoch 121/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 18.8051 - val_loss: 20.6772 - learning_rate: 1.0000e-04\n",
      "Epoch 122/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 19.5201 - val_loss: 20.4726 - learning_rate: 1.0000e-04\n",
      "Epoch 123/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 18.9912 - val_loss: 20.6946 - learning_rate: 1.0000e-04\n",
      "Epoch 124/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 19.1775 - val_loss: 20.4443 - learning_rate: 1.0000e-04\n",
      "Epoch 125/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 19.4236 - val_loss: 21.4120 - learning_rate: 1.0000e-04\n",
      "Epoch 126/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 18.3432 - val_loss: 20.6137 - learning_rate: 1.0000e-04\n",
      "Epoch 127/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 18.2101 - val_loss: 20.8469 - learning_rate: 1.0000e-04\n",
      "Epoch 128/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 19.6176 - val_loss: 20.4340 - learning_rate: 1.0000e-04\n",
      "Epoch 129/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 17.9442 - val_loss: 20.4702 - learning_rate: 1.0000e-04\n",
      "Epoch 130/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - loss: 18.6094 - val_loss: 20.8173 - learning_rate: 1.0000e-04\n",
      "Epoch 131/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step - loss: 20.2246 - val_loss: 20.1467 - learning_rate: 1.0000e-04\n",
      "Epoch 132/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - loss: 19.5282 - val_loss: 20.7263 - learning_rate: 1.0000e-04\n",
      "Epoch 133/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - loss: 18.6089 - val_loss: 20.2039 - learning_rate: 1.0000e-04\n",
      "Epoch 134/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - loss: 21.0983 - val_loss: 20.6814 - learning_rate: 1.0000e-04\n",
      "Epoch 135/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 19.1440 - val_loss: 20.3063 - learning_rate: 1.0000e-04\n",
      "Epoch 136/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 18.9542 - val_loss: 20.3944 - learning_rate: 1.0000e-04\n",
      "Epoch 137/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 19.9243 - val_loss: 21.9647 - learning_rate: 1.0000e-04\n",
      "Epoch 138/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 20.0092 - val_loss: 20.2285 - learning_rate: 1.0000e-04\n",
      "Epoch 139/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 21.3355 - val_loss: 20.3197 - learning_rate: 1.0000e-05\n",
      "Epoch 140/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 20.6133 - val_loss: 20.3502 - learning_rate: 1.0000e-05\n",
      "Epoch 141/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 20.7501 - val_loss: 20.3595 - learning_rate: 1.0000e-05\n",
      "Epoch 142/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 17.8047 - val_loss: 20.3872 - learning_rate: 1.0000e-05\n",
      "Epoch 143/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 17.7343 - val_loss: 20.4491 - learning_rate: 1.0000e-05\n",
      "Epoch 144/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 18.2116 - val_loss: 20.4091 - learning_rate: 1.0000e-05\n",
      "Epoch 145/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 18.9735 - val_loss: 20.3974 - learning_rate: 1.0000e-05\n",
      "Epoch 146/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 20.2693 - val_loss: 20.4313 - learning_rate: 1.0000e-05\n",
      "Epoch 147/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 17.8870 - val_loss: 20.4612 - learning_rate: 1.0000e-05\n",
      "Epoch 148/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - loss: 19.4084 - val_loss: 20.4287 - learning_rate: 1.0000e-05\n",
      "Epoch 149/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 19.7913 - val_loss: 20.4225 - learning_rate: 1.0000e-05\n",
      "Epoch 150/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - loss: 18.8253 - val_loss: 20.4320 - learning_rate: 1.0000e-05\n",
      "Epoch 151/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - loss: 17.2900 - val_loss: 20.4518 - learning_rate: 1.0000e-05\n",
      "Epoch 152/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - loss: 18.5557 - val_loss: 20.3984 - learning_rate: 1.0000e-05\n",
      "Epoch 153/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 19.3331 - val_loss: 20.4917 - learning_rate: 1.0000e-05\n",
      "Epoch 154/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - loss: 17.8284 - val_loss: 20.4759 - learning_rate: 1.0000e-05\n",
      "Epoch 155/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 21.1275 - val_loss: 20.5000 - learning_rate: 1.0000e-05\n",
      "Epoch 156/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 18.5849 - val_loss: 20.5221 - learning_rate: 1.0000e-05\n",
      "Epoch 157/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 19.0720 - val_loss: 20.4597 - learning_rate: 1.0000e-05\n",
      "Epoch 158/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 19.4597 - val_loss: 20.4429 - learning_rate: 1.0000e-05\n",
      "Epoch 159/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 21.7248 - val_loss: 20.5243 - learning_rate: 1.0000e-05\n",
      "Epoch 160/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 19.9770 - val_loss: 20.3916 - learning_rate: 1.0000e-05\n",
      "Epoch 161/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - loss: 17.4253 - val_loss: 20.4342 - learning_rate: 1.0000e-05\n",
      "Epoch 162/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 17.9098 - val_loss: 20.3586 - learning_rate: 1.0000e-05\n",
      "Epoch 163/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 17.8471 - val_loss: 20.3960 - learning_rate: 1.0000e-05\n",
      "Epoch 164/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 20.1015 - val_loss: 20.3942 - learning_rate: 1.0000e-05\n",
      "Epoch 165/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 19.0357 - val_loss: 20.4278 - learning_rate: 1.0000e-05\n",
      "Epoch 166/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 19.6828 - val_loss: 20.4115 - learning_rate: 1.0000e-05\n",
      "Epoch 167/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 19.3593 - val_loss: 20.4311 - learning_rate: 1.0000e-05\n",
      "Epoch 168/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 19.6915 - val_loss: 20.4186 - learning_rate: 1.0000e-05\n",
      "Epoch 169/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 18.3692 - val_loss: 20.4704 - learning_rate: 1.0000e-05\n",
      "Epoch 170/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step - loss: 19.1952 - val_loss: 20.4405 - learning_rate: 1.0000e-05\n",
      "Epoch 171/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 54ms/step - loss: 19.3236 - val_loss: 20.4695 - learning_rate: 1.0000e-05\n",
      "Epoch 172/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 21.0997 - val_loss: 20.4458 - learning_rate: 1.0000e-05\n",
      "Epoch 173/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 20.6614 - val_loss: 20.5169 - learning_rate: 1.0000e-05\n",
      "Epoch 174/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 19.0723 - val_loss: 20.5153 - learning_rate: 1.0000e-06\n",
      "Epoch 175/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - loss: 18.0261 - val_loss: 20.5095 - learning_rate: 1.0000e-06\n",
      "Epoch 176/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 18.6616 - val_loss: 20.4871 - learning_rate: 1.0000e-06\n",
      "Epoch 177/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 18.5251 - val_loss: 20.4831 - learning_rate: 1.0000e-06\n",
      "Epoch 178/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 16.5822 - val_loss: 20.4870 - learning_rate: 1.0000e-06\n",
      "Epoch 179/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 20.6800 - val_loss: 20.4791 - learning_rate: 1.0000e-06\n",
      "Epoch 180/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step - loss: 19.5924 - val_loss: 20.4651 - learning_rate: 1.0000e-06\n",
      "Epoch 181/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step - loss: 18.7085 - val_loss: 20.4763 - learning_rate: 1.0000e-06\n",
      "Epoch 182/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 20.8100 - val_loss: 20.4817 - learning_rate: 1.0000e-06\n",
      "Epoch 183/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - loss: 20.3153 - val_loss: 20.4658 - learning_rate: 1.0000e-06\n",
      "Epoch 184/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - loss: 19.9395 - val_loss: 20.4686 - learning_rate: 1.0000e-06\n",
      "Epoch 185/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - loss: 16.9394 - val_loss: 20.4580 - learning_rate: 1.0000e-06\n",
      "Epoch 186/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step - loss: 17.7451 - val_loss: 20.4415 - learning_rate: 1.0000e-06\n",
      "Epoch 187/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - loss: 19.3573 - val_loss: 20.4550 - learning_rate: 1.0000e-06\n",
      "Epoch 188/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - loss: 20.0473 - val_loss: 20.4489 - learning_rate: 1.0000e-06\n",
      "Epoch 189/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - loss: 17.4438 - val_loss: 20.4365 - learning_rate: 1.0000e-06\n",
      "Epoch 190/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - loss: 16.8822 - val_loss: 20.4486 - learning_rate: 1.0000e-06\n",
      "Epoch 191/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 57ms/step - loss: 18.2196 - val_loss: 20.4454 - learning_rate: 1.0000e-06\n",
      "Epoch 192/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - loss: 18.9195 - val_loss: 20.4548 - learning_rate: 1.0000e-06\n",
      "Epoch 193/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - loss: 19.3711 - val_loss: 20.4538 - learning_rate: 1.0000e-06\n",
      "Epoch 194/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - loss: 18.1331 - val_loss: 20.4565 - learning_rate: 1.0000e-06\n",
      "Epoch 195/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - loss: 18.4037 - val_loss: 20.4449 - learning_rate: 1.0000e-06\n",
      "Epoch 196/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - loss: 18.9405 - val_loss: 20.4442 - learning_rate: 1.0000e-06\n",
      "Epoch 197/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step - loss: 18.4465 - val_loss: 20.4446 - learning_rate: 1.0000e-06\n",
      "Epoch 198/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - loss: 19.0694 - val_loss: 20.4410 - learning_rate: 1.0000e-06\n",
      "Epoch 199/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - loss: 18.6309 - val_loss: 20.4423 - learning_rate: 1.0000e-06\n",
      "Epoch 200/200\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - loss: 19.0847 - val_loss: 20.4416 - learning_rate: 1.0000e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x13b2b9b6720>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "from  keras import callbacks\n",
    "\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n",
    "                              patience=35)\n",
    "\n",
    "model.fit(\n",
    "    [trainAttrX, trainImagesX], trainY,\n",
    "    validation_data=([testAttrX, testImagesX], testY),\n",
    "    epochs=200, batch_size=8, callbacks=[reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4398869c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 276ms/step\n"
     ]
    }
   ],
   "source": [
    "# make predictions on the testing data\n",
    "preds = model.predict([testAttrX, testImagesX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392f9559",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1c734859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] avg. house price: $533,388.27, std house price: $493,403.08\n",
      "[INFO] mean: 20.44%, std: 17.79%\n"
     ]
    }
   ],
   "source": [
    "# compute the difference between the *predicted* house prices and the\n",
    "# *actual* house prices, then compute the percentage difference and\n",
    "# the absolute percentage difference\n",
    "diff = preds.flatten() - testY\n",
    "percentDiff = (diff / testY) * 100\n",
    "absPercentDiff = np.abs(percentDiff)\n",
    "\n",
    "# compute the mean and standard deviation of the absolute percentage\n",
    "# difference\n",
    "mean = np.mean(absPercentDiff)\n",
    "std = np.std(absPercentDiff)\n",
    "\n",
    "# finally, show some statistics on our model\n",
    "locale.setlocale(locale.LC_ALL, \"en_US.UTF-8\")\n",
    "print(\"[INFO] avg. house price: {}, std house price: {}\".format(\n",
    "    locale.currency(df[\"price\"].mean(), grouping=True),\n",
    "    locale.currency(df[\"price\"].std(), grouping=True)))\n",
    "print(\"[INFO] mean: {:.2f}%, std: {:.2f}%\".format(mean, std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d3f52e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('C:\\\\Users\\\\Sepehr\\\\ML_file\\\\Deep-Learning\\\\BDI\\\\Houses-dataset\\\\myModel.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
